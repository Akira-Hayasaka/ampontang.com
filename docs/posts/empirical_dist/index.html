<!DOCTYPE html>

















<html lang="ja-JP">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

  
  <title>経験分布とは？ - Akira Hayasaka</title>

  
  
  <meta name="description" content="&ldquo;The Dirac Distribution and Empirical Distribution&rdquo; ↑ は、Goodfellow本 3.9.5節のタイトルです。Empirical Distribution が「経験分布」です。
経験分布はデータのサンプル（=観測・経験）集合の確率分布です。最尤推定はつまるところ、この経験分布と想定されるモデルの分布（正規分布、ベルヌーイ、カテゴリカルなど）の間のKLダイバージェンスを最小化するように訓練することです。そしてKLダイバージェンスを最小化するということは、交差エントロピーを最小化することと同じです。（この段でMSEは経験分布とガウス分布をモデルに仮定した場合の交差エントロピーだということがわかる）（ Goodfellow本 5.5節 あたりに書いてある)
何が言いたいかというと、経験分布とモデル分布の比較として最尤推定を捉えれば、回帰・2クラス分類・多クラス分類の”各種”誤差関数の根っこは同じになって、全部一貫して理解できるので、嬉しい、ということです。
ESLの2.4では統計的決定理論として、経験分布と予測分布（=&gt;モデル分布のこと）の誤差として、最小二乗法と最近傍法を一般化する形で説明されてます。
このあたりの首尾一貫した統計的学習理論は、僕みたいにエンジニアとして予備知識なしにハンズオン系の機械学習本から入った場合、少なくとも初回は全く素通りすることになってしまい、例えば最小二乗法と最近傍法は全く別の「ツール」として頭に入ってきて、先に進むにつれてそんなツールがひたすら増えて、もうダメです状態になります。何度か諦めながら色々と手を出しているうちに線がつながってくるのですが、結構時間も無駄にします。ただ、誰かが組んだカリキュラムに沿って最後まで登り切る忍耐もないし、結局いろんなところをうろうろしたうえで、来るべき時期が来ないとつながるべき線もつながらないという。まぁそういうもんですかね。。。
話がそれました。そんな経験分布ですが、
 経験分布は、 ディラック分布を構成要素としていて、 それはディラックのデルタ関数により確率密度関数として定義される  なのですが、読んでるだけだとそもそも３のディラックのデルタ関数のピンが来ない。理解のためにこんな感じかという実装をしてみました。
サンプルが正規分布してるとして、
import jax.numpy as jnp from jax import random import numpyro.distributions as dist import matplotlib.pyplot as plt; key = random.PRNGKey(49) normal = dist.Normal(loc=0.5, scale=1.) samples = normal.sample(key, (1000,)) plt.hist(samples) plt.show() このサンプルを確率分布にしたものが、経験分布です。なので、
def dirac_delta_fn(xx, sigma): val = [] for x in xx: if -(1 / (2 * sigma)) &lt;= x and x &lt;= (1 / (2 * sigma)): val." />
  <meta name="author" content="akira" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://akira-hayasaka.github.io/web/app.min.css" />

  

  
  <link rel="preload" as="image" href="https://akira-hayasaka.github.io/web/theme.png" />

  
  <link rel="preload" as="image" href="https://akira-hayasaka.github.io/web/twitter.svg" />
  
  <link rel="preload" as="image" href="https://akira-hayasaka.github.io/web/github.svg" />
  
  <link rel="preload" as="image" href="https://akira-hayasaka.github.io/web/instagram.svg" />
  

  
  <link rel="icon" href="https://akira-hayasaka.github.io/web/favicon.ico" />
  <link rel="apple-touch-icon" href="https://akira-hayasaka.github.io/web/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.91.2" />

  
  

  
  
  
  
  
  
  
  <meta property="og:title" content="経験分布とは？" />
<meta property="og:description" content="&ldquo;The Dirac Distribution and Empirical Distribution&rdquo; ↑ は、Goodfellow本 3.9.5節のタイトルです。Empirical Distribution が「経験分布」です。
経験分布はデータのサンプル（=観測・経験）集合の確率分布です。最尤推定はつまるところ、この経験分布と想定されるモデルの分布（正規分布、ベルヌーイ、カテゴリカルなど）の間のKLダイバージェンスを最小化するように訓練することです。そしてKLダイバージェンスを最小化するということは、交差エントロピーを最小化することと同じです。（この段でMSEは経験分布とガウス分布をモデルに仮定した場合の交差エントロピーだということがわかる）（ Goodfellow本 5.5節 あたりに書いてある)
何が言いたいかというと、経験分布とモデル分布の比較として最尤推定を捉えれば、回帰・2クラス分類・多クラス分類の”各種”誤差関数の根っこは同じになって、全部一貫して理解できるので、嬉しい、ということです。
ESLの2.4では統計的決定理論として、経験分布と予測分布（=&gt;モデル分布のこと）の誤差として、最小二乗法と最近傍法を一般化する形で説明されてます。
このあたりの首尾一貫した統計的学習理論は、僕みたいにエンジニアとして予備知識なしにハンズオン系の機械学習本から入った場合、少なくとも初回は全く素通りすることになってしまい、例えば最小二乗法と最近傍法は全く別の「ツール」として頭に入ってきて、先に進むにつれてそんなツールがひたすら増えて、もうダメです状態になります。何度か諦めながら色々と手を出しているうちに線がつながってくるのですが、結構時間も無駄にします。ただ、誰かが組んだカリキュラムに沿って最後まで登り切る忍耐もないし、結局いろんなところをうろうろしたうえで、来るべき時期が来ないとつながるべき線もつながらないという。まぁそういうもんですかね。。。
話がそれました。そんな経験分布ですが、
 経験分布は、 ディラック分布を構成要素としていて、 それはディラックのデルタ関数により確率密度関数として定義される  なのですが、読んでるだけだとそもそも３のディラックのデルタ関数のピンが来ない。理解のためにこんな感じかという実装をしてみました。
サンプルが正規分布してるとして、
import jax.numpy as jnp from jax import random import numpyro.distributions as dist import matplotlib.pyplot as plt; key = random.PRNGKey(49) normal = dist.Normal(loc=0.5, scale=1.) samples = normal.sample(key, (1000,)) plt.hist(samples) plt.show() このサンプルを確率分布にしたものが、経験分布です。なので、
def dirac_delta_fn(xx, sigma): val = [] for x in xx: if -(1 / (2 * sigma)) &lt;= x and x &lt;= (1 / (2 * sigma)): val." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://akira-hayasaka.github.io/web/posts/empirical_dist/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-01-07T12:59:05+09:00" />
<meta property="article:modified_time" content="2022-01-07T12:59:05+09:00" />


  
  <meta itemprop="name" content="経験分布とは？">
<meta itemprop="description" content="&ldquo;The Dirac Distribution and Empirical Distribution&rdquo; ↑ は、Goodfellow本 3.9.5節のタイトルです。Empirical Distribution が「経験分布」です。
経験分布はデータのサンプル（=観測・経験）集合の確率分布です。最尤推定はつまるところ、この経験分布と想定されるモデルの分布（正規分布、ベルヌーイ、カテゴリカルなど）の間のKLダイバージェンスを最小化するように訓練することです。そしてKLダイバージェンスを最小化するということは、交差エントロピーを最小化することと同じです。（この段でMSEは経験分布とガウス分布をモデルに仮定した場合の交差エントロピーだということがわかる）（ Goodfellow本 5.5節 あたりに書いてある)
何が言いたいかというと、経験分布とモデル分布の比較として最尤推定を捉えれば、回帰・2クラス分類・多クラス分類の”各種”誤差関数の根っこは同じになって、全部一貫して理解できるので、嬉しい、ということです。
ESLの2.4では統計的決定理論として、経験分布と予測分布（=&gt;モデル分布のこと）の誤差として、最小二乗法と最近傍法を一般化する形で説明されてます。
このあたりの首尾一貫した統計的学習理論は、僕みたいにエンジニアとして予備知識なしにハンズオン系の機械学習本から入った場合、少なくとも初回は全く素通りすることになってしまい、例えば最小二乗法と最近傍法は全く別の「ツール」として頭に入ってきて、先に進むにつれてそんなツールがひたすら増えて、もうダメです状態になります。何度か諦めながら色々と手を出しているうちに線がつながってくるのですが、結構時間も無駄にします。ただ、誰かが組んだカリキュラムに沿って最後まで登り切る忍耐もないし、結局いろんなところをうろうろしたうえで、来るべき時期が来ないとつながるべき線もつながらないという。まぁそういうもんですかね。。。
話がそれました。そんな経験分布ですが、
 経験分布は、 ディラック分布を構成要素としていて、 それはディラックのデルタ関数により確率密度関数として定義される  なのですが、読んでるだけだとそもそも３のディラックのデルタ関数のピンが来ない。理解のためにこんな感じかという実装をしてみました。
サンプルが正規分布してるとして、
import jax.numpy as jnp from jax import random import numpyro.distributions as dist import matplotlib.pyplot as plt; key = random.PRNGKey(49) normal = dist.Normal(loc=0.5, scale=1.) samples = normal.sample(key, (1000,)) plt.hist(samples) plt.show() このサンプルを確率分布にしたものが、経験分布です。なので、
def dirac_delta_fn(xx, sigma): val = [] for x in xx: if -(1 / (2 * sigma)) &lt;= x and x &lt;= (1 / (2 * sigma)): val."><meta itemprop="datePublished" content="2022-01-07T12:59:05+09:00" />
<meta itemprop="dateModified" content="2022-01-07T12:59:05+09:00" />
<meta itemprop="wordCount" content="136">
<meta itemprop="keywords" content="機械学習,Machine Learning," />
  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="経験分布とは？"/>
<meta name="twitter:description" content="&ldquo;The Dirac Distribution and Empirical Distribution&rdquo; ↑ は、Goodfellow本 3.9.5節のタイトルです。Empirical Distribution が「経験分布」です。
経験分布はデータのサンプル（=観測・経験）集合の確率分布です。最尤推定はつまるところ、この経験分布と想定されるモデルの分布（正規分布、ベルヌーイ、カテゴリカルなど）の間のKLダイバージェンスを最小化するように訓練することです。そしてKLダイバージェンスを最小化するということは、交差エントロピーを最小化することと同じです。（この段でMSEは経験分布とガウス分布をモデルに仮定した場合の交差エントロピーだということがわかる）（ Goodfellow本 5.5節 あたりに書いてある)
何が言いたいかというと、経験分布とモデル分布の比較として最尤推定を捉えれば、回帰・2クラス分類・多クラス分類の”各種”誤差関数の根っこは同じになって、全部一貫して理解できるので、嬉しい、ということです。
ESLの2.4では統計的決定理論として、経験分布と予測分布（=&gt;モデル分布のこと）の誤差として、最小二乗法と最近傍法を一般化する形で説明されてます。
このあたりの首尾一貫した統計的学習理論は、僕みたいにエンジニアとして予備知識なしにハンズオン系の機械学習本から入った場合、少なくとも初回は全く素通りすることになってしまい、例えば最小二乗法と最近傍法は全く別の「ツール」として頭に入ってきて、先に進むにつれてそんなツールがひたすら増えて、もうダメです状態になります。何度か諦めながら色々と手を出しているうちに線がつながってくるのですが、結構時間も無駄にします。ただ、誰かが組んだカリキュラムに沿って最後まで登り切る忍耐もないし、結局いろんなところをうろうろしたうえで、来るべき時期が来ないとつながるべき線もつながらないという。まぁそういうもんですかね。。。
話がそれました。そんな経験分布ですが、
 経験分布は、 ディラック分布を構成要素としていて、 それはディラックのデルタ関数により確率密度関数として定義される  なのですが、読んでるだけだとそもそも３のディラックのデルタ関数のピンが来ない。理解のためにこんな感じかという実装をしてみました。
サンプルが正規分布してるとして、
import jax.numpy as jnp from jax import random import numpyro.distributions as dist import matplotlib.pyplot as plt; key = random.PRNGKey(49) normal = dist.Normal(loc=0.5, scale=1.) samples = normal.sample(key, (1000,)) plt.hist(samples) plt.show() このサンプルを確率分布にしたものが、経験分布です。なので、
def dirac_delta_fn(xx, sigma): val = [] for x in xx: if -(1 / (2 * sigma)) &lt;= x and x &lt;= (1 / (2 * sigma)): val."/>

  
  
</head>


  <body class="not-ready" data-menu="true">
    <header class="header">
  
  <p class="logo">
    <a class="site-name" href="https://akira-hayasaka.github.io/web/">Akira Hayasaka</a><a class="btn-dark"></a>
  </p>
  

  <script>
    let bodyClx = document.body.classList;
    let btnDark = document.querySelector('.btn-dark');
    let sysDark = window.matchMedia('(prefers-color-scheme: dark)');
    let darkVal = localStorage.getItem('dark');

    let setDark = (isDark) => {
      bodyClx[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark ? 'yes' : 'no');
    };

    setDark(darkVal ? darkVal === 'yes' : sysDark.matches);
    requestAnimationFrame(() => bodyClx.remove('not-ready'));

    btnDark.addEventListener('click', () => setDark(!bodyClx.contains('dark')));
    sysDark.addEventListener('change', (event) => setDark(event.matches));
  </script>

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
  

  
  
  <nav class="menu">
    
    <a class="" href="https://akira-hayasaka.github.io/web/about/">about</a>
    
    <a class="" href="https://akira-hayasaka.github.io/web/work/">work</a>
    
  </nav>
  

  
  <nav class="social">
    
    <a
      class="twitter"
      style="--url: url(./twitter.svg)"
      href="https://twitter.com/Akira_At_Asia"
      target="_blank"
    ></a>
    
    <a
      class="github"
      style="--url: url(./github.svg)"
      href="https://github.com/Akira-Hayasaka"
      target="_blank"
    ></a>
    
    <a
      class="instagram"
      style="--url: url(./instagram.svg)"
      href="https://instagram.com/akira.hysk"
      target="_blank"
    ></a>
    
  </nav>
  
</header>


    <main class="main">

<article class="post-single">
  <header class="post-title">
    <p>
      <time>Jan 7, 2022</time>
      
      <span>akira</span>
      
    </p>
    <h1>経験分布とは？</h1>
  </header>
  <section class="post-content"><h1 id="the-dirac-distribution-and-empirical-distribution">&ldquo;The Dirac Distribution and Empirical Distribution&rdquo;</h1>
<p>↑ は、<a href="https://www.deeplearningbook.org/contents/prob.html">Goodfellow本 3.9.5節</a>のタイトルです。Empirical Distribution が「経験分布」です。</p>
<p>経験分布はデータのサンプル（=観測・経験）集合の確率分布です。最尤推定はつまるところ、この経験分布と想定されるモデルの分布（正規分布、ベルヌーイ、カテゴリカルなど）の間のKLダイバージェンスを最小化するように訓練することです。そしてKLダイバージェンスを最小化するということは、交差エントロピーを最小化することと同じです。（この段でMSEは経験分布とガウス分布をモデルに仮定した場合の交差エントロピーだということがわかる）（ <a href="https://www.deeplearningbook.org/contents/ml.html">Goodfellow本 5.5節</a> あたりに書いてある)</p>
<p>何が言いたいかというと、経験分布とモデル分布の比較として最尤推定を捉えれば、回帰・2クラス分類・多クラス分類の”各種”誤差関数の根っこは同じになって、全部一貫して理解できるので、嬉しい、ということです。</p>
<p><a href="https://www.amazon.co.jp/%E7%B5%B1%E8%A8%88%E7%9A%84%E5%AD%A6%E7%BF%92%E3%81%AE%E5%9F%BA%E7%A4%8E-%E2%80%95%E3%83%87%E3%83%BC%E3%82%BF%E3%83%9E%E3%82%A4%E3%83%8B%E3%83%B3%E3%82%B0%E3%83%BB%E6%8E%A8%E8%AB%96%E3%83%BB%E4%BA%88%E6%B8%AC%E2%80%95-Trevor-Hastie/dp/432012362X/ref=sr_1_1?adgrpid=73121729924&amp;gclid=Cj0KCQiAoY-PBhCNARIsABcz773CoHk8_rfSNDRupEaGt6GY-zhqME-uzM32RsvJ5JK5J2CeFppdi58aAudYEALw_wcB&amp;hvadid=353826168644&amp;hvdev=c&amp;hvlocphy=1009301&amp;hvnetw=g&amp;hvqmt=e&amp;hvrand=5666640908829365875&amp;hvtargid=kwd-848857017784&amp;hydadcr=13360_11184391&amp;jp-ad-ap=0&amp;keywords=%E7%B5%B1%E8%A8%88%E7%9A%84%E5%AD%A6%E7%BF%92%E3%81%AE%E5%9F%BA%E7%A4%8E&amp;qid=1642387895&amp;sr=8-1">ESLの2.4</a>では統計的決定理論として、経験分布と予測分布（=&gt;モデル分布のこと）の誤差として、最小二乗法と最近傍法を一般化する形で説明されてます。</p>
<p>このあたりの首尾一貫した統計的学習理論は、僕みたいにエンジニアとして予備知識なしにハンズオン系の機械学習本から入った場合、少なくとも初回は全く素通りすることになってしまい、例えば最小二乗法と最近傍法は全く別の「ツール」として頭に入ってきて、先に進むにつれてそんなツールがひたすら増えて、もうダメです状態になります。何度か諦めながら色々と手を出しているうちに線がつながってくるのですが、結構時間も無駄にします。ただ、誰かが組んだカリキュラムに沿って最後まで登り切る忍耐もないし、結局いろんなところをうろうろしたうえで、来るべき時期が来ないとつながるべき線もつながらないという。まぁそういうもんですかね。。。</p>
<p>話がそれました。そんな経験分布ですが、</p>
<ol>
<li>経験分布は、</li>
<li>ディラック分布を構成要素としていて、</li>
<li>それはディラックのデルタ関数により確率密度関数として定義される</li>
</ol>
<p>なのですが、読んでるだけだとそもそも３のディラックのデルタ関数のピンが来ない。理解のためにこんな感じかという実装をしてみました。</p>
<p>サンプルが正規分布してるとして、</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> jax.numpy <span style="color:#66d9ef">as</span> jnp
<span style="color:#f92672">from</span> jax <span style="color:#f92672">import</span> random
<span style="color:#f92672">import</span> numpyro.distributions <span style="color:#66d9ef">as</span> dist
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt; 
key <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>PRNGKey(<span style="color:#ae81ff">49</span>)
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">normal <span style="color:#f92672">=</span> dist<span style="color:#f92672">.</span>Normal(loc<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1.</span>)
samples <span style="color:#f92672">=</span> normal<span style="color:#f92672">.</span>sample(key, (<span style="color:#ae81ff">1000</span>,))
plt<span style="color:#f92672">.</span>hist(samples)
plt<span style="color:#f92672">.</span>show()
</code></pre></div><p><img src="https://akira-hayasaka.github.io/web/posts/empirical_dist/output.png" alt="dist"></p>
<p>このサンプルを確率分布にしたものが、経験分布です。なので、</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">dirac_delta_fn</span>(xx, sigma):
   val <span style="color:#f92672">=</span> []
   <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> xx:
       <span style="color:#66d9ef">if</span> <span style="color:#f92672">-</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> sigma)) <span style="color:#f92672">&lt;=</span> x <span style="color:#f92672">and</span> x <span style="color:#f92672">&lt;=</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> sigma)):
           val<span style="color:#f92672">.</span>append(sigma)
       <span style="color:#66d9ef">else</span>:
           val<span style="color:#f92672">.</span>append(<span style="color:#ae81ff">0</span>)
   <span style="color:#66d9ef">return</span> jnp<span style="color:#f92672">.</span>array(val)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">pick_prob_of_x</span>(current_x, samples, sigma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>):
    <span style="color:#66d9ef">return</span> jnp<span style="color:#f92672">.</span>sum(dirac_delta_fn([current_x <span style="color:#f92672">-</span> sample <span style="color:#66d9ef">for</span> sample <span style="color:#f92672">in</span> samples], sigma)) <span style="color:#f92672">/</span> samples<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]    

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">form_empirical_distribution</span>(_from, _to, samples, step<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>):
    xx <span style="color:#f92672">=</span> []
    yy <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> current_x <span style="color:#f92672">in</span> np<span style="color:#f92672">.</span>arange(_from, _to, step):
        y <span style="color:#f92672">=</span> pick_prob_of_x(current_x, samples)
        xx<span style="color:#f92672">.</span>append(current_x)
        yy<span style="color:#f92672">.</span>append(y)
    <span style="color:#66d9ef">return</span> xx, yy

xx, yy <span style="color:#f92672">=</span> form_empirical_distribution(<span style="color:#f92672">-</span><span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, samples)

plt<span style="color:#f92672">.</span>plot(xx, yy)
plt<span style="color:#f92672">.</span>show()
</code></pre></div><p><img src="https://akira-hayasaka.github.io/web/posts/empirical_dist/output2.png" alt="dist"></p>
<p>サンプルだけから確率分布を作ることができるのであった。</p>
</section>

  
  
  <footer class="post-tags">
     
    <a href="https://akira-hayasaka.github.io/web/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92">機械学習</a>
     
    <a href="https://akira-hayasaka.github.io/web/tags/machine-learning">Machine Learning</a>
    
  </footer>
  

  
  
  
  <nav class="post-nav">
    
    <a class="prev" href="https://akira-hayasaka.github.io/web/posts/bayesian_linreg/"><span>←</span><span>numpyroでベイズ線形回帰</span></a>
     
    <a class="next" href="https://akira-hayasaka.github.io/web/posts/hello/"><span>Hello</span><span>→</span></a>
    
  </nav>
  

  
  
</article>

</main>

    <footer class="footer">
  <p>&copy; 2022 <a href="https://akira-hayasaka.github.io/web/">Akira Hayasaka</a></p>
  <p>Powered by <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>️</p>
  <p>
    <a href="https://github.com/nanxiaobei/hugo-paper" rel="noopener" target="_blank">Paper 5.1</a>
  </p>
</footer>

  </body>
</html>
