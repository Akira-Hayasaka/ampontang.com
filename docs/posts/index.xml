<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Akira Hayasaka</title>
    <link>https://akira-hayasaka.github.io/web/posts/</link>
    <description>Recent content in Posts on Akira Hayasaka</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-JP</language>
    <lastBuildDate>Sun, 09 Jan 2022 14:23:39 +0900</lastBuildDate><atom:link href="https://akira-hayasaka.github.io/web/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>線形因子モデル</title>
      <link>https://akira-hayasaka.github.io/web/posts/linear_dim_reduction/</link>
      <pubDate>Sun, 09 Jan 2022 14:23:39 +0900</pubDate>
      
      <guid>https://akira-hayasaka.github.io/web/posts/linear_dim_reduction/</guid>
      <description>線形因子モデル 線形因子モデルは、goodfellow本ではその後に続く深層生成モデルの、須山本では第五章の応用モデルの、基礎を成しているとのことなので、やってみました。モデルとしては、PRMLの12章を参考にしています。
線形因子モデルは、潜在変数に情報を詰め込んで、そこからデータを再現します。
 PPCA, factor analysisは大体これと同じ ICAはローカルの潜在変数に独立を仮定し、ガウス分布以外の物を使う  ので、goodfellow本13章で紹介されている発展的なモデルも、これができればそんなに遠くない（はず）。
潜在変数の解釈 潜在変数というのは、深層学習の文脈で表現（representation）や特徴（feature）と呼ばれるもので、
 ベイズ系の統計推測での潜在変数 NNが隠れ層で学習する多様体 オートエンコーダの隠れ層 スパースコーディングの成果物 教師なし学習で事前学習する（していた）際の目的 CNNのフィルタが学習するもの RNNが時系列で共有するもの（パラメータシェアリング）  などなどは、同じイデアを共有していて、緩やかに繋がっているものだと理解しています。深層学習ではその潜在変数（=表現・特徴）がスパースであるほど統計的に意義がある（uniformに分布してると何も言えないから）かつその潜在変数を使った後続の回帰や分類タスクの精度が上がるので、そのスパース性を求めて様々な正則化が適用されます。
この潜在変数をいかにうまく設計するかが、統計的推論・統計的機械学習のキモです。
実装 グラフィカルモデル 観測データ $$ \quad Y=[y_1,&amp;hellip;,y_n] \quad y_n \in \mathbb{R}^D $$
潜在変数 $$ \quad X=[x_1,&amp;hellip;,x_n] \quad x_n \in \mathbb{R}^M \\ \quad \textbf{W} \in \mathbb{R}^{M \times D} \quad (\textbf{W}_d \in \mathbb{R}^M \quad W の d 番⽬の列ベクトル)\\ \quad \mu \in \mathbb{R}^D $$
パラメータ $$ \quad \sigma^2_y \in \mathbb{R}^+ \\ \Sigma_w \\ \Sigma_{\mu} $$</description>
    </item>
    
    <item>
      <title>numpyroでベイズ線形回帰</title>
      <link>https://akira-hayasaka.github.io/web/posts/bayesian_linreg/</link>
      <pubDate>Fri, 07 Jan 2022 14:32:23 +0900</pubDate>
      
      <guid>https://akira-hayasaka.github.io/web/posts/bayesian_linreg/</guid>
      <description>線形回帰は全ての基本です。
変分推論のロジックがわかったら、エグい計算はライブラリを使って避けたいです。
model $$ \begin{align*} y_n &amp;amp; \in \mathbb{R} \qquad (output) \\ x_n &amp;amp; \in \mathbb{R}^M \quad e.g. (1,x,x^2,x^3) \qquad (input) \\ \textbf{w} &amp;amp; \in \mathbb{R}^M \qquad (parameter) \\ \epsilon_n &amp;amp; \in \mathbb{R} \qquad (noise) \end{align*} $$
$$ y_n = \textbf{w}^T\textbf{x}_n + \epsilon_n \\ \epsilon_n \sim Norm(\epsilon_n|0,\lambda^{-1}) $$
$$ p(\textbf{t},\textbf{w}|\textbf{x},\alpha^{-1},\beta^{-1}) = p(\textbf{w}|\alpha^{-1})\prod^{\textit{N}}_{n=1}p(\textit{t}_n|\textbf{w},\textit{x}_n,\beta^{-1}) $$
$$ = Norm(\textbf{w}|0,\alpha^{-1})\prod^{\textit{N}}_{n=1}Norm(t_n|\textbf{w}^T\phi(\textit{x}_n),\beta^{-1}) $$
scipy for modeling M = 3 def phi_basis_fn(x): result = [] result.append(1.0) for i in range(M-1): result.</description>
    </item>
    
    <item>
      <title>経験分布とは？</title>
      <link>https://akira-hayasaka.github.io/web/posts/empirical_dist/</link>
      <pubDate>Fri, 07 Jan 2022 12:59:05 +0900</pubDate>
      
      <guid>https://akira-hayasaka.github.io/web/posts/empirical_dist/</guid>
      <description>&amp;ldquo;The Dirac Distribution and Empirical Distribution&amp;rdquo; ↑ は、Goodfellow本 3.9.5節のタイトルです。Empirical Distribution が「経験分布」です。
経験分布はデータのサンプル（=観測・経験）集合の確率分布です。最尤推定はつまるところ、この経験分布と想定されるモデルの分布（正規分布、ベルヌーイ、カテゴリカルなど）の間のKLダイバージェンスを最小化するように訓練することです。そしてKLダイバージェンスを最小化するということは、交差エントロピーを最小化することと同じです。（この段でMSEは経験分布とガウス分布をモデルに仮定した場合の交差エントロピーだということがわかる）（ Goodfellow本 5.5節 あたりに書いてある)
何が言いたいかというと、経験分布とモデル分布の比較として最尤推定を捉えれば、回帰・2クラス分類・多クラス分類の”各種”誤差関数の根っこは同じになって、全部一貫して理解できるので、嬉しい、ということです。
そんな経験分布ですが、
 経験分布は、 ディラック分布を構成要素としていて、 それはディラックのデルタ関数により確率密度関数として定義される  なのですが、読んでるだけだとそもそも３のディラックのデルタ関数のピンが来ない。理解のためにこんな感じかという実装をしてみました。
サンプルが正規分布してるとして、
import jax.numpy as jnp from jax import random import numpyro.distributions as dist import matplotlib.pyplot as plt; key = random.PRNGKey(49) normal = dist.Normal(loc=0.5, scale=1.) samples = normal.sample(key, (1000,)) plt.hist(samples) plt.show() このサンプルを確率分布にしたものが、経験分布です。なので、
def dirac_delta_fn(xx, sigma): val = [] for x in xx: if -(1 / (2 * sigma)) &amp;lt;= x and x &amp;lt;= (1 / (2 * sigma)): val.</description>
    </item>
    
    <item>
      <title>Hello</title>
      <link>https://akira-hayasaka.github.io/web/posts/hello/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:22 +0900</pubDate>
      
      <guid>https://akira-hayasaka.github.io/web/posts/hello/</guid>
      <description>備忘録、近況報告のために書きます。
技術的なことを話したり、デモを投稿したり、デジタル系施策・企画に何か話したりする思います。最近ずっと統計的推測・機械学習まわりをやっているので、その成長記録としても残していきます。</description>
    </item>
    
    <item>
      <title>テスト1</title>
      <link>https://akira-hayasaka.github.io/web/posts/test/</link>
      <pubDate>Wed, 06 Jan 2021 12:23:01 +0900</pubDate>
      
      <guid>https://akira-hayasaka.github.io/web/posts/test/</guid>
      <description>テスト tttsssttt
Headings blah
H1 blah
H6 blah
Paragraph blah
Tables    Name Age     Bob 27   Alice 23    Inline Markdown within tables    Italics Bold Code     italics bold code    Code Blocks Code block with backticks int main() { int y = SOME_MACRO_REFERENCE; int x = 5 + 6; cout &amp;lt;&amp;lt; &amp;#34;Hello World!</description>
    </item>
    
  </channel>
</rss>
