---
title: "線形因子モデル"
date: 2022-01-07T16:53:39+09:00
draft: false
tags: ["統計的推測", "numpyro"]
author: "akira"
---

線形因子モデルは、[goodfellow本](https://www.deeplearningbook.org/)ではその後に続く深層生成モデルの、[須山本](https://www.amazon.co.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E3%83%99%E3%82%A4%E3%82%BA%E6%8E%A8%E8%AB%96%E3%81%AB%E3%82%88%E3%82%8B%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E5%85%A5%E9%96%80-KS%E6%83%85%E5%A0%B1%E7%A7%91%E5%AD%A6%E5%B0%82%E9%96%80%E6%9B%B8-%E9%A0%88%E5%B1%B1-%E6%95%A6%E5%BF%97/dp/4061538322/ref=sr_1_1?keywords=%E3%83%99%E3%82%A4%E3%82%BA%E6%8E%A8%E8%AB%96%E3%81%AB%E3%82%88%E3%82%8B%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E5%85%A5%E9%96%80&qid=1641542845&sprefix=%E3%83%99%E3%82%A4%E3%82%BA%E6%8E%A8%E8%AB%96%2Caps%2C185&sr=8-1)では第五章の応用モデルの、基礎を成しているとのことなので、やってみました。モデルとしては、[PRML](https://www.amazon.co.jp/%E3%83%91%E3%82%BF%E3%83%BC%E3%83%B3%E8%AA%8D%E8%AD%98%E3%81%A8%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-%E4%B8%8B-%E3%83%99%E3%82%A4%E3%82%BA%E7%90%86%E8%AB%96%E3%81%AB%E3%82%88%E3%82%8B%E7%B5%B1%E8%A8%88%E7%9A%84%E4%BA%88%E6%B8%AC-C-M-%E3%83%93%E3%82%B7%E3%83%A7%E3%83%83%E3%83%97/dp/4621061240/ref=pd_lpo_1?pd_rd_i=4621061240&psc=1)の12章を参考にしています。

隠れ変数に情報を詰め込んで、そこからデータを再現します。

- PPCA, factor analysisは大体これと同じ
- ICAはローカルの潜在変数に独立を仮定し、ガウス分布以外の物を使う

ので、これができればそんなに遠くない（はず）。

隠れ変数というのは、深層学習の文脈で表現（representation）や特徴（feature）と呼ばれるもので、
- ベイズ系の統計推測での隠れ変数
- NNが隠れ層で学習する[多様体](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)
- オートエンコーダの隠れ層
- スパースコーディングの成果物
- 教師なし学習で事前学習する（していた）際の目的
- CNNのフィルタが学習するもの
- RNNが時系列で共有するもの（パラメータシェアリング）

などなどは、同じイデアを共有していて、緩やかに繋がっているものだと理解しています。深層学習ではその隠れ変数（=表現・特徴）がスパースであるほど統計的に意義がある（uniformに分布してると何も言えないから）かつその隠れ変数を使った後続の回帰や分類タスクの精度が上がるので、そのスパース性を求めて様々な正則化が適用されます。